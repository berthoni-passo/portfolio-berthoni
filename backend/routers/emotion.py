from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import boto3
import base64
import os

router = APIRouter(
    prefix="/api/ml",
    tags=["ML"]
)

class ImagePayload(BaseModel):
    image_base64: str  # base64 de l'image captur√©e depuis le navigateur

# Mapping √©motions ‚Üí emoji
EMOTION_EMOJI = {
    "HAPPY": "üòÑ",
    "SAD": "üò¢",
    "ANGRY": "üò†",
    "SURPRISED": "üò≤",
    "CALM": "üòê",
    "DISGUSTED": "ü§¢",
    "CONFUSED": "üòï",
    "FEAR": "üò®"
}

@router.post("/emotion")
def detect_emotion(payload: ImagePayload):
    """
    Re√ßoit une image en base64, appelle AWS Rekognition detect_faces,
    retourne les √©motions avec leurs scores de confiance.
    """
    # 1. D√©coder l'image base64
    try:
        # Supprimer le prefixe data URL si pr√©sent (ex: "data:image/jpeg;base64,...")
        image_data = payload.image_base64
        if "," in image_data:
            image_data = image_data.split(",")[1]
        image_bytes = base64.b64decode(image_data)
    except Exception:
        raise HTTPException(status_code=400, detail="Image base64 invalide")

    # 2. Appel AWS Rekognition
    try:
        client = boto3.client(
            "rekognition",
            region_name=os.getenv("AWS_REKOGNITION_REGION", "eu-west-1"),  # Rekognition non dispo en eu-west-3
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY")
        )
        response = client.detect_faces(
            Image={"Bytes": image_bytes},
            Attributes=["ALL"]
        )
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Erreur AWS Rekognition : {str(e)}")

    # 3. V√©rifier qu'un visage est d√©tect√©
    face_details = response.get("FaceDetails", [])
    if not face_details:
        raise HTTPException(status_code=422, detail="Aucun visage d√©tect√© dans l'image")

    # 4. Prendre le premier visage (le plus confiant)
    face = face_details[0]

    # 5. Extraire et trier les √©motions
    emotions_raw = face.get("Emotions", [])
    emotions = sorted(emotions_raw, key=lambda e: e["Confidence"], reverse=True)

    emotions_formatted = [
        {
            "type": e["Type"],
            "emoji": EMOTION_EMOJI.get(e["Type"], "üôÇ"),
            "confidence": round(e["Confidence"], 1)
        }
        for e in emotions
    ]

    # 6. Infos bonus du visage
    age_range = face.get("AgeRange", {})
    gender = face.get("Gender", {})
    smile = face.get("Smile", {})
    eyeglasses = face.get("Eyeglasses", {})

    return {
        "dominant_emotion": emotions_formatted[0] if emotions_formatted else None,
        "emotions": emotions_formatted,
        "face_info": {
            "age_range": {
                "low": age_range.get("Low"),
                "high": age_range.get("High")
            },
            "gender": gender.get("Value"),
            "gender_confidence": round(gender.get("Confidence", 0), 1),
            "smile": smile.get("Value", False),
            "eyeglasses": eyeglasses.get("Value", False)
        },
        "faces_detected": len(face_details)
    }
